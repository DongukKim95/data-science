{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool, Value, freeze_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>키워드</th>\n",
       "      <th>일자</th>\n",
       "      <th>채널</th>\n",
       "      <th>제목</th>\n",
       "      <th>작성자(언론사명)</th>\n",
       "      <th>작성일자</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>종로</td>\n",
       "      <td>20190630</td>\n",
       "      <td>ko.news</td>\n",
       "      <td>[LCK 서머] '바이퍼' 박도현 \"리라, 어떤 게임 될지 기대되고 설레\"</td>\n",
       "      <td>포모스</td>\n",
       "      <td>2.019060e+13</td>\n",
       "      <td>https://sports.news.naver.com/esports/news/rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>종로</td>\n",
       "      <td>20190630</td>\n",
       "      <td>ko.news</td>\n",
       "      <td>[롤챔스] '압도적인 힘' 그리핀, 킹존 격파하고 선두 재탈환(종합)</td>\n",
       "      <td>OSEN</td>\n",
       "      <td>2.019060e+13</td>\n",
       "      <td>https://sports.news.naver.com/esports/news/rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>종로</td>\n",
       "      <td>20190630</td>\n",
       "      <td>ko.news</td>\n",
       "      <td>다시 1위 도약한 그리핀...2019 LCK 서머 순위현황(6월 30일)</td>\n",
       "      <td>포모스</td>\n",
       "      <td>2.019060e+13</td>\n",
       "      <td>https://sports.news.naver.com/esports/news/rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>종로</td>\n",
       "      <td>20190630</td>\n",
       "      <td>ko.news</td>\n",
       "      <td>[LCK 서머] 운영-전투 균형 이룬 그리핀, 킹존에 2대 0완승(종합)</td>\n",
       "      <td>포모스</td>\n",
       "      <td>2.019060e+13</td>\n",
       "      <td>https://sports.news.naver.com/esports/news/rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>종로</td>\n",
       "      <td>20190630</td>\n",
       "      <td>ko.news</td>\n",
       "      <td>[롤챔스] 킹존-그리핀, LCK 서머 스플릿 15번째 매진</td>\n",
       "      <td>OSEN</td>\n",
       "      <td>2.019060e+13</td>\n",
       "      <td>https://sports.news.naver.com/esports/news/rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  키워드        일자       채널                                         제목 작성자(언론사명)  \\\n",
       "0  종로  20190630  ko.news  [LCK 서머] '바이퍼' 박도현 \"리라, 어떤 게임 될지 기대되고 설레\"       포모스   \n",
       "1  종로  20190630  ko.news     [롤챔스] '압도적인 힘' 그리핀, 킹존 격파하고 선두 재탈환(종합)      OSEN   \n",
       "2  종로  20190630  ko.news   다시 1위 도약한 그리핀...2019 LCK 서머 순위현황(6월 30일)       포모스   \n",
       "3  종로  20190630  ko.news   [LCK 서머] 운영-전투 균형 이룬 그리핀, 킹존에 2대 0완승(종합)       포모스   \n",
       "4  종로  20190630  ko.news           [롤챔스] 킹존-그리핀, LCK 서머 스플릿 15번째 매진      OSEN   \n",
       "\n",
       "           작성일자                                                URL  \n",
       "0  2.019060e+13  https://sports.news.naver.com/esports/news/rea...  \n",
       "1  2.019060e+13  https://sports.news.naver.com/esports/news/rea...  \n",
       "2  2.019060e+13  https://sports.news.naver.com/esports/news/rea...  \n",
       "3  2.019060e+13  https://sports.news.naver.com/esports/news/rea...  \n",
       "4  2.019060e+13  https://sports.news.naver.com/esports/news/rea...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daum_data = pd.read_csv('금융빅데이터공모전_데이터/다음소프트_원문정보.csv', encoding='cp949')\n",
    "daum_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in daum_data.columns:\n",
    "    print('------------', col, '|------------')\n",
    "    print(daum_data[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "종로      45888\n",
       "을지로     30993\n",
       "중구      28582\n",
       "익선동     27025\n",
       "서촌      12522\n",
       "힙지로      6324\n",
       "중리단길       26\n",
       "Name: 키워드, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daum_data[daum_data.채널 == 'ko.insta'].키워드.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('insta_crawling_서촌.txt', 'r', encoding='utf-8')\n",
    "len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter URL array\n",
    "\n",
    "url_arr = daum_data[daum_data.채널 == 'ko.twitter'].URL.values\n",
    "url_arr[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter text crawling test\n",
    "\n",
    "if True:\n",
    "    keyword = '익선동'\n",
    "    keyword_url_arr = daum_data[(daum_data.채널 == 'ko.twitter') & (daum_data.키워드 == keyword)].URL.values\n",
    "    \n",
    "    f = open('tweets_crawling_{}.txt'.format(keyword), 'w', encoding='utf-8')\n",
    "\n",
    "    for url in keyword_url_arr:\n",
    "        try:\n",
    "            raw = requests.get(url, headers = {'User-agent' : 'Mozilla/5.0'})\n",
    "            html = BeautifulSoup(raw.text, 'html.parser')\n",
    "            container = html.select_one('div.permalink-inner.permalink-tweet-container > div > div.js-tweet-text-container > p')\n",
    "            text = container.text\n",
    "            text = re.sub(\"[\\n\\u200b]\", \" \", text)  # \\n, \\u 공백으로 변경 \n",
    "            text = re.sub(\"\\s+\", \" \", text)  # 여러 공백 하나로 통일\n",
    "            text = re.sub(\"[http://]\\w*\",\"\", text)  # 주소 제거\n",
    "            text = re.sub(\"[_]?[pic.twitter.com]\\w*\", \"\", text)  # twitter 주소 제거\n",
    "            # print(text)\n",
    "            try:\n",
    "                f.write(text+'\\n')\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서촌\n",
      "을지로\n",
      "익선동\n",
      "종로\n",
      "중구\n",
      "중리단길\n",
      "힙지로\n"
     ]
    }
   ],
   "source": [
    "for k in np.unique(daum_data.키워드.values):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter text crawling\n",
    "\n",
    "for keyword in np.unique(daum_data.키워드.values):\n",
    "    keyword_url_arr = daum_data[(daum_data.채널 == 'ko.twitter') & (daum_data.키워드 == keyword)].URL.values\n",
    "    \n",
    "    f = open('tweets_crawling_{}.txt'.format(keyword), 'w', encoding='utf-8')\n",
    "    \n",
    "    print('------------------{}-----------------'.format(keyword))\n",
    "    \n",
    "    for url in keyword_url_arr:\n",
    "        try:\n",
    "            raw = requests.get(url, headers = {'User-agent' : 'Mozilla/5.0'})\n",
    "            html = BeautifulSoup(raw.text, 'html.parser')\n",
    "            container = html.select_one('div.permalink-inner.permalink-tweet-container > div > div.js-tweet-text-container > p')\n",
    "            text = container.text\n",
    "            text = re.sub(\"[\\n\\u200b]\", \" \", text)  # \\n, \\u 공백으로 변경\n",
    "            text = re.sub(\"\\s+\", \" \", text)  # 여러 공백 하나로 통일\n",
    "            text = re.sub(\"[http://]\\w*\",\"\", text)  # 주소 제거\n",
    "            text = re.sub(\"[_]?[pic.twitter.com]\\w*\", \"\", text)  # twitter 주소 제거\n",
    "            try:\n",
    "                f.write(text+'\\n')\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium 속도 향상용\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "prefs = {'profile.default_content_setting_values': {'cookies': 2, 'images': 2, 'javascript': 2, \n",
    "                            'plugins': 2, 'popups': 2, 'geolocation': 2, \n",
    "                            'notifications': 2, 'auto_select_certificate': 2, 'fullscreen': 2, \n",
    "                            'mouselock': 2, 'mixed_script': 2, 'media_stream': 2, \n",
    "                            'media_stream_mic': 2, 'media_stream_camera': 2, 'protocol_handlers': 2, \n",
    "                            'ppapi_broker': 2, 'automatic_downloads': 2, 'midi_sysex': 2, \n",
    "                            'push_messaging': 2, 'ssl_cert_decisions': 2, 'metro_switch_to_desktop': 2, \n",
    "                            'protected_media_identifier': 2, 'app_banner': 2, 'site_engagement': 2, \n",
    "                            'durable_storage': 2}}\n",
    "options.add_experimental_option('prefs', prefs)\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"disable-infobars\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "# 크롬창을 띄우지 않는 옵션을 넣는다\t\n",
    "options.add_argument('headless')\t\n",
    "options.add_argument('disable-gpu')\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------힙지로-----------------\n"
     ]
    }
   ],
   "source": [
    "# instagram text crawling code\n",
    "\n",
    "for k in np.unique(daum_data.키워드.values)[6:]:\n",
    "    keyword = k\n",
    "    number = 0\n",
    "    keyword_url_arr = daum_data[(daum_data.채널 == 'ko.insta') & (\n",
    "        daum_data.키워드 == keyword)].URL.values\n",
    "   \n",
    "    f = open('insta_crawling_{}.txt'.format(keyword), 'w', encoding='utf-8')\n",
    "    \n",
    "    print('------------------{}-----------------'.format(keyword))\n",
    "    \n",
    "    for url in keyword_url_arr:\n",
    "        try:\n",
    "            driver = webdriver.Chrome('./chromedriver', options=options)\n",
    "            driver.get(url)\n",
    "\n",
    "            taglist = driver.find_elements_by_xpath(\n",
    "                \"//meta[@property='instapp:hashtags']\")\n",
    "\n",
    "            text = ' '.join([i.get_attribute(\"content\") for i in taglist])\n",
    "            # print(text)\n",
    "            try:\n",
    "                if text != '':  # 데이터가 있으면 파일에 추가\n",
    "                    f.write(text+'\\n')\n",
    "                    number += 1\n",
    "                    if number >= 8000:\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing\n",
    "# instagram text crawling code\n",
    "# 실행 안 됨 => 어떻게 수정해야하지..\n",
    "\n",
    "global f\n",
    "\n",
    "def crawling(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome('./chromedriver', options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        taglist = driver.find_elements_by_xpath(\n",
    "                \"//meta[@property='instapp:hashtags']\")\n",
    "\n",
    "        tag_list = []\n",
    "        tag_list = [i.get_attribute(\"content\") for i in taglist]\n",
    "        text = ' '.join(tag_list)\n",
    "        print(tag_list)\n",
    "        print(text)\n",
    "        try:\n",
    "            if tag_list != [] and text != '':  # 데이터가 있으면 파일에 추가\n",
    "                f.write(text+'\\n')\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for k in np.unique(daum_data.키워드.values)[0:1]:\n",
    "    keyword = k\n",
    "    keyword_url_arr = daum_data[(daum_data.채널 == 'ko.insta') & (\n",
    "        daum_data.키워드 == keyword)].URL.values\n",
    "    \n",
    "    f = open('insta_crawling_{}.txt'.format(keyword), 'w', encoding='utf-8')\n",
    "    \n",
    "    print('------------------{}-----------------'.format(keyword))\n",
    "    \n",
    "    p = Pool(4)\n",
    "    p.map(crawling, keyword_url_arr[:10])\n",
    "    p.close()\n",
    "       \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 core 실행\n",
    "\n",
    "p = Pool(4)\n",
    "p.map(crawling, keyword_url_arr)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = '익선동'\n",
    "keyword_url_arr = daum_data[(daum_data.채널 == 'ko.insta') & (daum_data.키워드 == keyword)].URL.values\n",
    "\n",
    "url = keyword_url_arr[3]\n",
    "\n",
    "crawling(keyword_url_arr[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
